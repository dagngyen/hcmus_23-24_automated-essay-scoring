{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        padding: 8px 8px;\n",
    "        background-image: linear-gradient(135deg, #c9f3ff, rgb(131, 218, 255));\n",
    "        font-weight: 700;\n",
    "        position: static;\n",
    "        text-align: center;\n",
    "        color: #006098;\n",
    "        font-size: 36px;\n",
    "    }\n",
    "    h2 {\n",
    "        font-weight: 700;\n",
    "        text-align: center;\n",
    "        font-style: italic;\n",
    "        font-size: 24px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div><h1>XÂY DỰNG MODEL</h1></div>\n",
    "<div><h2>SVM model via SVR</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khai báo thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dagngyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m140.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "# ! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "! pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "# ! pip install spacy\n",
    "from spacy import load\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
    "# train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/test.csv')\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9016</th>\n",
       "      <td>8525a80</td>\n",
       "      <td>Dear senator,\\n\\nI am writing to you from the ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>b595104</td>\n",
       "      <td>To Whom It May Concern,\\n\\nThe Electoral Colle...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14212</th>\n",
       "      <td>d129f6b</td>\n",
       "      <td>Sure, driverless cars sound interesting, but a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>15ec1e4</td>\n",
       "      <td>Why We Should Study Venus\\n\\nWhat is venus lik...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>6fd79d3</td>\n",
       "      <td>The author in \"Driverless Cars Are Coming\" pre...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  score\n",
       "9016   8525a80  Dear senator,\\n\\nI am writing to you from the ...      3\n",
       "12327  b595104  To Whom It May Concern,\\n\\nThe Electoral Colle...      4\n",
       "14212  d129f6b  Sure, driverless cars sound interesting, but a...      4\n",
       "1401   15ec1e4  Why We Should Study Venus\\n\\nWhat is venus lik...      4\n",
       "7461   6fd79d3  The author in \"Driverless Cars Are Coming\" pre...      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text\n",
       "0  000d118  Many people have car where they live. The thin...\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...\n",
       "2  001ab80  People always wish they had the same technolog..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>7570efd</td>\n",
       "      <td>Do you believe in aliens or life on other plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>56ae3e1</td>\n",
       "      <td>The Challenge of Exploring Venus, a article ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>31e00b5</td>\n",
       "      <td>\"With less cars we have less accidents and les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>6f7ae5d</td>\n",
       "      <td>I do not agree with the idea of completey driv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16692</th>\n",
       "      <td>f60205d</td>\n",
       "      <td>The Facial Action Coding System created by Dr....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text\n",
       "7829   7570efd  Do you believe in aliens or life on other plan...\n",
       "5780   56ae3e1  The Challenge of Exploring Venus, a article ab...\n",
       "3359   31e00b5  \"With less cars we have less accidents and les...\n",
       "7436   6f7ae5d  I do not agree with the idea of completey driv...\n",
       "16692  f60205d  The Facial Action Coding System created by Dr...."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove score from train and concate with test\n",
    "all_data = pd.concat([train.iloc[:,:-1], test], axis=0).reset_index(drop=True)\n",
    "all_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ loại bỏ các ký tự dư thừa và chuỗi không cung cấp nhiều ý nghĩa thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_str(text):\n",
    "    # Loại bỏ các username bắt đầu @\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    # Loại bỏ các thẻ HTML\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)\n",
    "    # Loại bỏ URL\n",
    "    text = [word for word in text.split() if not urlparse(word).scheme]\n",
    "    text = ' '.join(text)\n",
    "    # Loại bỏ dấu nháy đơn mà theo sau nó là chữ số\n",
    "    text = re.sub(\"'\\d+\", \"\", text)\n",
    "    # Loại bỏ các ký tự dư thừa như khoảng trắng, dấu chấm, dấu phẩy\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trích lọc đặc trưng từ dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta định nghĩa và cài đặt hàm đếm số lượng từ vựng bị sử dụng sai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spelling_errors(word_list):\n",
    "    return len(list(SpellChecker().unknown(word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, định nghĩa hàm loại bỏ dấu câu để thống kê thông số về từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, cài đặt hàm phân tách thành các đoạn văn, thành các câu để thống kê."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(text):\n",
    "    new_text = re.split('\\n\\n', text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cài đặt hàm trích lọc những từ khóa đóng góp quan trọng trong ý nghĩa của câu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text\n",
    "def filter_word(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần này, ta sẽ quan sát những giá trị thống kê từ văn bản:\n",
    "- Tổng số lượng đoạn văn;\n",
    "- Thống kê theo đoạn văn: Sau khi phân tách văn bản và xử lý dữ liệu, ta sẽ thống kê:\n",
    "    + Số lượng câu: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Tổng số lượng câu;\n",
    "- Thống kê theo câu: Sau khi phân tách thành các câu và xử lý dữ liệu, ta sẽ thống kê:\n",
    "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Tổng số lượng từ;\n",
    "- Tổng số lượng từ sau khi trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn;\n",
    "- Thống kê theo từ: Trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn:\n",
    "    + Số lượng ký tự: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Số lượng từ loại sai chính tả trong đoạn văn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_engineering(paragraph):\n",
    "    _paragraph = list(map(remove_excess_str, paragraph))\n",
    "    _lst_sents = []\n",
    "    _lst_words = []\n",
    "    for para in _paragraph:\n",
    "        _lst_sents.append(len(sent_tokenize(para)))\n",
    "        _lst_words.append(len(word_tokenize(para)))\n",
    "    return len(_paragraph),\\\n",
    "            np.min(_lst_sents), np.quantile(_lst_sents, 0.25), np.median(_lst_sents), np.quantile(_lst_sents, 0.75), np.max(_lst_sents),\\\n",
    "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['num_para'],\\\n",
    "    all_data['para_min_sent'], all_data['para_q1_sent'], all_data['para_median_sent'], all_data['para_q3_sent'], all_data['para_max_sent'],\\\n",
    "    all_data['para_min_word'], all_data['para_q1_word'], all_data['para_median_word'], all_data['para_q3_word'], all_data['para_max_word'] = zip(*all_data['full_text'].map(split_paragraphs).map(paragraph_engineering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_engineering(sentence):\n",
    "    _lst_words = []\n",
    "    for sent in sentence:\n",
    "        _lst_words.append(len(word_tokenize(sent)))\n",
    "    return len(sentence),\\\n",
    "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['num_sent'],\\\n",
    "    all_data['sent_min_word'], all_data['sent_q1_word'], all_data['sent_median_word'], all_data['sent_q3_word'], all_data['sent_max_word']\\\n",
    "        = zip(*all_data['full_text'].apply(remove_excess_str).map(sent_tokenize).map(sentence_engineering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_engineering(word):\n",
    "    _lst_chars = list(map(len, word))\n",
    "    return len(word), np.min(_lst_chars), np.quantile(_lst_chars, 0.25), np.median(_lst_chars), np.quantile(_lst_chars, 0.75), np.max(_lst_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_full_text = all_data['full_text'].map(remove_excess_str).map(remove_punctuation)\n",
    "word_full_text_no_stopwords = word_full_text.map(filter_word)\n",
    "# all_data['num_error'] = word_full_text.map(word_tokenize).map(count_spelling_errors)\n",
    "all_data['num_word'] = word_full_text.map(word_tokenize).map(len)\n",
    "all_data['num_word_no_stopwords'], all_data['word_min_chars'], all_data['word_q1_chars'], all_data['word_median_chars'], all_data['word_q3_chars'], all_data['word_max_chars'] = zip(*word_full_text_no_stopwords.map(word_engineering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Số lượng đặc trung sau khi rút trích thông tin từ dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17310, 26)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape[1] - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trích lọc đặc trưng thông qua TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"TF-IDF là viết tắt của “Term Frequency, Inverse Document Frequency” - tạm dịch “Tần suất thuật ngữ, Tần suất tài liệu nghịch đảo”. Đó là một cách để chấm điểm tầm quan trọng của các từ (hoặc \\\"các thuật ngữ\\\") dựa trên tần suất xuất hiện của chúng xuất hiện trên nhiều tài liệu dựa trên quy tắc sau:\"\n",
    "- Nếu một từ xuất hiện thường xuyên trong tài liệu, điều đó rất quan trọng. Cho từ này điểm cao.\n",
    "- Nhưng nếu một từ xuất hiện trong nhiều tài liệu, thì đó không phải là mã định danh duy nhất. Cho từ đó điểm thấp.\n",
    "\n",
    "Do đó, những từ phổ biến như `the` và `for` xuất hiện trong nhiều tài liệu sẽ được scaled down. Các từ xuất hiện thường xuyên trong một tài liệu sẽ được scaled up.\n",
    "\n",
    "Với những giải thích trên, ta có công thức tính trọng số của một từ trong tài liệu trong ngữ liệu như sau:\n",
    "$$w_{i,j} = tf_{i,j} \\cdot idf_i = tf_{i,j} \\cdot log(\\frac {N}{df_i})$$\n",
    "\n",
    "Trong đó:\n",
    "- $tf_{i,j}$: Tần suất xuất hiện của i trong j\n",
    "- $N$: Tổng số tài liệu\n",
    "- $df_i$: Số tài liệu chứa i\n",
    "\n",
    "**Reference:** https://medium.com/analytics-vidhya/an-introduction-to-tf-idf-using-python-5f9d1a343f77\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo TfidfVectorizer với các tham số cụ thể\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=0.05,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# Sử dụng TfidfVectorizer cho dữ liệu\n",
    "tfidf = vectorizer.fit_transform([row for row in all_data['full_text']]).toarray()\n",
    "\n",
    "# Lưu kết quả\n",
    "colums_tfidf = [f'tfidf_{i}' for i in range(tfidf.shape[1])]\n",
    "tfidf_df = pd.DataFrame(tfidf, columns=colums_tfidf)\n",
    "tfidf_df['essay_id'] = all_data['essay_id']\n",
    "all_data = all_data.merge(tfidf_df, on='essay_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>num_para</th>\n",
       "      <th>para_min_sent</th>\n",
       "      <th>para_q1_sent</th>\n",
       "      <th>para_median_sent</th>\n",
       "      <th>para_q3_sent</th>\n",
       "      <th>para_max_sent</th>\n",
       "      <th>para_min_word</th>\n",
       "      <th>para_q1_word</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_3282</th>\n",
       "      <th>tfidf_3283</th>\n",
       "      <th>tfidf_3284</th>\n",
       "      <th>tfidf_3285</th>\n",
       "      <th>tfidf_3286</th>\n",
       "      <th>tfidf_3287</th>\n",
       "      <th>tfidf_3288</th>\n",
       "      <th>tfidf_3289</th>\n",
       "      <th>tfidf_3290</th>\n",
       "      <th>tfidf_3291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>545</td>\n",
       "      <td>545.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>545</td>\n",
       "      <td>545.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>104.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17311</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>545</td>\n",
       "      <td>545.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17312</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17313</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17314</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>104.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17315</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>104.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17316 rows × 3318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  num_para  \\\n",
       "0      000d118  Many people have car where they live. The thin...         1   \n",
       "1      000d118  Many people have car where they live. The thin...         1   \n",
       "2      000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "3      000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "4      001ab80  People always wish they had the same technolog...         4   \n",
       "...        ...                                                ...       ...   \n",
       "17311  000d118  Many people have car where they live. The thin...         1   \n",
       "17312  000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "17313  000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "17314  001ab80  People always wish they had the same technolog...         4   \n",
       "17315  001ab80  People always wish they had the same technolog...         4   \n",
       "\n",
       "       para_min_sent  para_q1_sent  para_median_sent  para_q3_sent  \\\n",
       "0                 13          13.0              13.0          13.0   \n",
       "1                 13          13.0              13.0          13.0   \n",
       "2                  2           3.0               3.0           5.0   \n",
       "3                  2           3.0               3.0           5.0   \n",
       "4                  4           4.0               5.5           7.5   \n",
       "...              ...           ...               ...           ...   \n",
       "17311             13          13.0              13.0          13.0   \n",
       "17312              2           3.0               3.0           5.0   \n",
       "17313              2           3.0               3.0           5.0   \n",
       "17314              4           4.0               5.5           7.5   \n",
       "17315              4           4.0               5.5           7.5   \n",
       "\n",
       "       para_max_sent  para_min_word  para_q1_word  ...  tfidf_3282  \\\n",
       "0                 13            545        545.00  ...         0.0   \n",
       "1                 13            545        545.00  ...         0.0   \n",
       "2                  8             44         52.00  ...         0.0   \n",
       "3                  8             44         52.00  ...         0.0   \n",
       "4                  9             92        104.75  ...         0.0   \n",
       "...              ...            ...           ...  ...         ...   \n",
       "17311             13            545        545.00  ...         0.0   \n",
       "17312              8             44         52.00  ...         0.0   \n",
       "17313              8             44         52.00  ...         0.0   \n",
       "17314              9             92        104.75  ...         0.0   \n",
       "17315              9             92        104.75  ...         0.0   \n",
       "\n",
       "       tfidf_3283  tfidf_3284  tfidf_3285  tfidf_3286  tfidf_3287  tfidf_3288  \\\n",
       "0             0.0         0.0         0.0         0.0         0.0    0.034733   \n",
       "1             0.0         0.0         0.0         0.0         0.0    0.034733   \n",
       "2             0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "3             0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "4             0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "17311         0.0         0.0         0.0         0.0         0.0    0.034733   \n",
       "17312         0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "17313         0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "17314         0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "17315         0.0         0.0         0.0         0.0         0.0    0.000000   \n",
       "\n",
       "       tfidf_3289  tfidf_3290  tfidf_3291  \n",
       "0        0.071065         0.0         0.0  \n",
       "1        0.071065         0.0         0.0  \n",
       "2        0.000000         0.0         0.0  \n",
       "3        0.000000         0.0         0.0  \n",
       "4        0.000000         0.0         0.0  \n",
       "...           ...         ...         ...  \n",
       "17311    0.071065         0.0         0.0  \n",
       "17312    0.000000         0.0         0.0  \n",
       "17313    0.000000         0.0         0.0  \n",
       "17314    0.000000         0.0         0.0  \n",
       "17315    0.000000         0.0         0.0  \n",
       "\n",
       "[17316 rows x 3318 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
