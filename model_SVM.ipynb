{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iam-Dylan/automated-essay-scoring/blob/dagngyen/model_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5XItd6cqqk4"
      },
      "source": [
        "<style>\n",
        "    h1 {\n",
        "        padding: 8px 8px;\n",
        "        background-image: linear-gradient(135deg, #c9f3ff, rgb(131, 218, 255));\n",
        "        font-weight: 700;\n",
        "        position: static;\n",
        "        text-align: center;\n",
        "        color: #006098;\n",
        "        font-size: 36px;\n",
        "    }\n",
        "    h2 {\n",
        "        font-weight: 700;\n",
        "        text-align: center;\n",
        "        font-style: italic;\n",
        "        font-size: 24px;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div><h1>XÂY DỰNG MODEL</h1></div>\n",
        "<div><h2>SVM model via SVR</h2></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW3sVEfgqqk5"
      },
      "source": [
        "# Khai báo thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIZhHmPjqqk6",
        "outputId": "72b4ac43-5327-40d5-e09b-a9a67e3a5d34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/seaborn/_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.1)\n",
            "  from scipy.stats import gaussian_kde\n",
            "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/dagngyen/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/dagngyen/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (0.8.1)\n",
            "Requirement already satisfied: spacy in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (69.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from spacy) (1.22.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "! pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "! pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "! pip install spacy\n",
        "from spacy import load\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import cohen_kappa_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXTF3Yasqqk6"
      },
      "source": [
        "# Đọc dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUNqOF5Fqqk7"
      },
      "source": [
        "- Đọc dữ liệu trên Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnNcn8MJqqk7",
        "outputId": "06785edf-f45f-4407-9441-10338b0ac6ec"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iw3JxBnQqqk7",
        "outputId": "1c486cd3-5647-4fcb-bc1e-329ec64e274b"
      },
      "outputs": [],
      "source": [
        "# train = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
        "# train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Cds8yFewqqk7",
        "outputId": "8c6a44f3-4195-4a7f-83f1-0b50f2bd1fd0"
      },
      "outputs": [],
      "source": [
        "# test = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/test.csv')\n",
        "# test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAlpajnRqqk7"
      },
      "source": [
        "- Đọc dữ liệu trên local path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uQtW7fU2qqk7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15124</th>\n",
              "      <td>deb6eb4</td>\n",
              "      <td>There are a few advantage of limiting car usag...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14344</th>\n",
              "      <td>d35875f</td>\n",
              "      <td>I think this Facial Action Coding System would...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8279</th>\n",
              "      <td>7b7b596</td>\n",
              "      <td>Can you imagine a time in the future when no o...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4680</th>\n",
              "      <td>45c449d</td>\n",
              "      <td>One day luke and don wanted another job so the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9951</th>\n",
              "      <td>9326b22</td>\n",
              "      <td>I think that the seagoing cowboy program sound...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      essay_id                                          full_text  score\n",
              "15124  deb6eb4  There are a few advantage of limiting car usag...      2\n",
              "14344  d35875f  I think this Facial Action Coding System would...      2\n",
              "8279   7b7b596  Can you imagine a time in the future when no o...      2\n",
              "4680   45c449d  One day luke and don wanted another job so the...      1\n",
              "9951   9326b22  I think that the seagoing cowboy program sound...      3"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
        "train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ww9zbUmJqqk7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000d118</td>\n",
              "      <td>Many people have car where they live. The thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fe60</td>\n",
              "      <td>I am a scientist at NASA that is discussing th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>001ab80</td>\n",
              "      <td>People always wish they had the same technolog...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  essay_id                                          full_text\n",
              "0  000d118  Many people have car where they live. The thin...\n",
              "1  000fe60  I am a scientist at NASA that is discussing th...\n",
              "2  001ab80  People always wish they had the same technolog..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BS_KulkHqqk8",
        "outputId": "716065d4-a366-4bb8-cb9f-efbd1c88188b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13961</th>\n",
              "      <td>cd08ad7</td>\n",
              "      <td>Can a computer recognize the subtke facial mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7283</th>\n",
              "      <td>6d24baf</td>\n",
              "      <td>Technology cannot read emotinal expresseions b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9917</th>\n",
              "      <td>92a3030</td>\n",
              "      <td>The author in this story supports his details ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13278</th>\n",
              "      <td>c3058fe</td>\n",
              "      <td>The Face on Mars\\n\\nBy jimmy\\n\\nNASA`s had sen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8913</th>\n",
              "      <td>83b0019</td>\n",
              "      <td>\"So Mr. Ashwalt now that you've read the artic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      essay_id                                          full_text\n",
              "13961  cd08ad7  Can a computer recognize the subtke facial mov...\n",
              "7283   6d24baf  Technology cannot read emotinal expresseions b...\n",
              "9917   92a3030  The author in this story supports his details ...\n",
              "13278  c3058fe  The Face on Mars\\n\\nBy jimmy\\n\\nNASA`s had sen...\n",
              "8913   83b0019  \"So Mr. Ashwalt now that you've read the artic..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove score from train and concate with test\n",
        "all_data = pd.concat([train.iloc[:,:-1], test], axis=0).reset_index(drop=True)\n",
        "all_data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pW2gAp2qqk8",
        "outputId": "24dbac7c-34a2-45d2-cd96-9f0b52474c27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17310, 2)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Cs84FHqqk8"
      },
      "source": [
        "# Xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMI0hZJLqqk8"
      },
      "source": [
        "Ta sẽ loại bỏ các ký tự dư thừa và chuỗi không cung cấp nhiều ý nghĩa thông tin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uucY3aEvqqk8"
      },
      "outputs": [],
      "source": [
        "def remove_excess_str(text):\n",
        "    # Chuyển chữ viết hoa thành chữ thường\n",
        "    text = text.lower()\n",
        "    # Loại bỏ các username bắt đầu @\n",
        "    text = re.sub(\"@\\w+\", '', text)\n",
        "    # Xóa hashtag (dấu #)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Loại bỏ các thẻ HTML\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub(r'', text)\n",
        "    # Loại bỏ URL\n",
        "    text = [word for word in text.split() if not urlparse(word).scheme]\n",
        "    text = ' '.join(text)\n",
        "    # Loại bỏ dấu nháy đơn mà theo sau nó là chữ số\n",
        "    text = re.sub(\"'\\d+\", \"\", text)\n",
        "    # Loại bỏ các ký tự dư thừa như khoảng trắng, dấu chấm, dấu phẩy\n",
        "    text = re.sub(r\"\\.+\", \".\", text)\n",
        "    text = re.sub(r\"\\,+\", \",\", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUUy_pMqqk9"
      },
      "source": [
        "# Trích lọc đặc trưng từ dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRwDNUqXqqk9"
      },
      "source": [
        "Đầu tiên, ta định nghĩa và cài đặt hàm đếm số lượng từ vựng bị sử dụng sai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hzjGNJUNqqk9"
      },
      "outputs": [],
      "source": [
        "def count_spelling_errors(word_list):\n",
        "    return len(list(SpellChecker().unknown(word_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f-BpJcoqqk9"
      },
      "source": [
        "Tiếp theo, định nghĩa hàm loại bỏ dấu câu để thống kê thông số về từ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IqB-D7X_qqk9"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSgP18yiqqk9"
      },
      "source": [
        "Tiếp theo, cài đặt hàm phân tách thành các đoạn văn, thành các câu để thống kê."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8gTjfELSqqk9"
      },
      "outputs": [],
      "source": [
        "def split_paragraphs(text):\n",
        "    new_text = re.split('\\n\\n', text)\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqEBIJf2qqk9"
      },
      "source": [
        "Cài đặt hàm trích lọc những từ khóa đóng góp quan trọng trong ý nghĩa của câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pSaGmzjIqqk9"
      },
      "outputs": [],
      "source": [
        "# Load Spacy model\n",
        "nlp = load(\"en_core_web_sm\")\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocess the text\n",
        "def filter_word(text):\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jDuBbUIqqk9"
      },
      "source": [
        "Phần này, ta sẽ quan sát những giá trị thống kê từ văn bản:\n",
        "- Tổng số lượng đoạn văn;\n",
        "- Thống kê theo đoạn văn: Sau khi phân tách văn bản và xử lý dữ liệu, ta sẽ thống kê:\n",
        "    + Số lượng câu: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
        "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
        "- Tổng số lượng câu;\n",
        "- Thống kê theo câu: Sau khi phân tách thành các câu và xử lý dữ liệu, ta sẽ thống kê:\n",
        "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
        "- Tổng số lượng từ;\n",
        "- Tổng số lượng từ sau khi trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn;\n",
        "- Thống kê theo từ: Trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn:\n",
        "    + Số lượng ký tự: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
        "- Số lượng từ loại sai chính tả trong đoạn văn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4PQVTMF5qqk9"
      },
      "outputs": [],
      "source": [
        "def paragraph_engineering(paragraph):\n",
        "    _paragraph = list(map(remove_excess_str, paragraph))\n",
        "    _lst_sents = []\n",
        "    _lst_words = []\n",
        "    for para in _paragraph:\n",
        "        _lst_sents.append(len(sent_tokenize(para)))\n",
        "        _lst_words.append(len(word_tokenize(para)))\n",
        "    return len(_paragraph),\\\n",
        "            np.min(_lst_sents), np.quantile(_lst_sents, 0.25), np.median(_lst_sents), np.quantile(_lst_sents, 0.75), np.max(_lst_sents),\\\n",
        "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kg8HKuRdqqk-"
      },
      "outputs": [],
      "source": [
        "all_data['num_para'],\\\n",
        "    all_data['para_min_sent'], all_data['para_q1_sent'], all_data['para_median_sent'], all_data['para_q3_sent'], all_data['para_max_sent'],\\\n",
        "    all_data['para_min_word'], all_data['para_q1_word'], all_data['para_median_word'], all_data['para_q3_word'], all_data['para_max_word'] = zip(*all_data['full_text'].map(split_paragraphs).map(paragraph_engineering))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kLXYfSGCqqk-"
      },
      "outputs": [],
      "source": [
        "def sentence_engineering(sentence):\n",
        "    _lst_words = []\n",
        "    for sent in sentence:\n",
        "        _lst_words.append(len(word_tokenize(sent)))\n",
        "    return len(sentence),\\\n",
        "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kNRki5S-qqk-"
      },
      "outputs": [],
      "source": [
        "all_data['num_sent'],\\\n",
        "    all_data['sent_min_word'], all_data['sent_q1_word'], all_data['sent_median_word'], all_data['sent_q3_word'], all_data['sent_max_word']\\\n",
        "        = zip(*all_data['full_text'].apply(remove_excess_str).map(sent_tokenize).map(sentence_engineering))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mlkVcbizqqk-"
      },
      "outputs": [],
      "source": [
        "def word_engineering(word):\n",
        "    _lst_chars = list(map(len, word))\n",
        "    return len(word), np.min(_lst_chars), np.quantile(_lst_chars, 0.25), np.median(_lst_chars), np.quantile(_lst_chars, 0.75), np.max(_lst_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3CsYU79-qqk-"
      },
      "outputs": [],
      "source": [
        "word_full_text = all_data['full_text'].map(remove_excess_str).map(remove_punctuation)\n",
        "word_full_text_no_stopwords = word_full_text.map(filter_word)\n",
        "# all_data['num_error'] = word_full_text.map(word_tokenize).map(count_spelling_errors)\n",
        "all_data['num_word'] = word_full_text.map(word_tokenize).map(len)\n",
        "all_data['num_word_no_stopwords'], all_data['word_min_chars'], all_data['word_q1_chars'], all_data['word_median_chars'], all_data['word_q3_chars'], all_data['word_max_chars'] = zip(*word_full_text_no_stopwords.map(word_engineering))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoOrFs0gqqk-"
      },
      "source": [
        "Số lượng đặc trung sau khi rút trích thông tin từ dữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V2kzjAbqqk-",
        "outputId": "163f00b1-5875-4a7b-c40d-48b3966ae0a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17310, 26)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-AWqFkdqqk-"
      },
      "source": [
        "# Trích lọc đặc trưng thông qua TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJvdLgZMqqk-"
      },
      "source": [
        "\"TF-IDF là viết tắt của “Term Frequency, Inverse Document Frequency” - tạm dịch “Tần suất thuật ngữ, Tần suất tài liệu nghịch đảo”. Đó là một cách để chấm điểm tầm quan trọng của các từ (hoặc \\\"các thuật ngữ\\\") dựa trên tần suất xuất hiện của chúng xuất hiện trên nhiều tài liệu dựa trên quy tắc sau:\"\n",
        "- Nếu một từ xuất hiện thường xuyên trong tài liệu, điều đó rất quan trọng. Cho từ này điểm cao.\n",
        "- Nhưng nếu một từ xuất hiện trong nhiều tài liệu, thì đó không phải là mã định danh duy nhất. Cho từ đó điểm thấp.\n",
        "\n",
        "Do đó, những từ phổ biến như `the` và `for` xuất hiện trong nhiều tài liệu sẽ được scaled down. Các từ xuất hiện thường xuyên trong một tài liệu sẽ được scaled up.\n",
        "\n",
        "Với những giải thích trên, ta có công thức tính trọng số của một từ trong tài liệu trong ngữ liệu như sau:\n",
        "$$w_{i,j} = tf_{i,j} \\cdot idf_i = tf_{i,j} \\cdot log(\\frac {N}{df_i})$$\n",
        "\n",
        "Trong đó:\n",
        "- $tf_{i,j}$: Tần suất xuất hiện của i trong j\n",
        "- $N$: Tổng số tài liệu\n",
        "- $df_i$: Số tài liệu chứa i\n",
        "\n",
        "**Reference:** https://medium.com/analytics-vidhya/an-introduction-to-tf-idf-using-python-5f9d1a343f77\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0XOiHjyQqqk-"
      },
      "outputs": [],
      "source": [
        "# Khởi tạo TfidfVectorizer với các tham số cụ thể\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=lambda x: x,\n",
        "    preprocessor=lambda x: x,\n",
        "    token_pattern=None,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    ngram_range=(1, 3),\n",
        "    min_df=0.05,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True,\n",
        ")\n",
        "\n",
        "# Sử dụng TfidfVectorizer cho dữ liệu\n",
        "tfidf = vectorizer.fit_transform([row for row in all_data['full_text']]).toarray()\n",
        "\n",
        "# Lưu kết quả\n",
        "colums_tfidf = [f'tfidf_{i}' for i in range(tfidf.shape[1])]\n",
        "tfidf_df = pd.DataFrame(tfidf, columns=colums_tfidf)\n",
        "all_data = pd.concat([all_data, tfidf_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "73Z4MiVkqqk_",
        "outputId": "26afff20-877f-4a1f-b79e-ce6b7c99be0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>num_para</th>\n",
              "      <th>para_min_sent</th>\n",
              "      <th>para_q1_sent</th>\n",
              "      <th>para_median_sent</th>\n",
              "      <th>para_q3_sent</th>\n",
              "      <th>para_max_sent</th>\n",
              "      <th>para_min_word</th>\n",
              "      <th>para_q1_word</th>\n",
              "      <th>...</th>\n",
              "      <th>tfidf_3282</th>\n",
              "      <th>tfidf_3283</th>\n",
              "      <th>tfidf_3284</th>\n",
              "      <th>tfidf_3285</th>\n",
              "      <th>tfidf_3286</th>\n",
              "      <th>tfidf_3287</th>\n",
              "      <th>tfidf_3288</th>\n",
              "      <th>tfidf_3289</th>\n",
              "      <th>tfidf_3290</th>\n",
              "      <th>tfidf_3291</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000d118</td>\n",
              "      <td>Many people have car where they live. The thin...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>13.00</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13</td>\n",
              "      <td>545</td>\n",
              "      <td>545.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034733</td>\n",
              "      <td>0.071065</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fe60</td>\n",
              "      <td>I am a scientist at NASA that is discussing th...</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8</td>\n",
              "      <td>44</td>\n",
              "      <td>52.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>001ab80</td>\n",
              "      <td>People always wish they had the same technolog...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.00</td>\n",
              "      <td>5.5</td>\n",
              "      <td>7.5</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>104.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>001bdc0</td>\n",
              "      <td>We all heard about Venus, the planet without a...</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7</td>\n",
              "      <td>25</td>\n",
              "      <td>69.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043294</td>\n",
              "      <td>0.057518</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>002ba53</td>\n",
              "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.25</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>22.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17305</th>\n",
              "      <td>fffb49b</td>\n",
              "      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11</td>\n",
              "      <td>264</td>\n",
              "      <td>264.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17306</th>\n",
              "      <td>fffed3e</td>\n",
              "      <td>Venus is worthy place to study but dangerous. ...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>16.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17307</th>\n",
              "      <td>000d118</td>\n",
              "      <td>Many people have car where they live. The thin...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>13.00</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13</td>\n",
              "      <td>545</td>\n",
              "      <td>545.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034733</td>\n",
              "      <td>0.071065</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17308</th>\n",
              "      <td>000fe60</td>\n",
              "      <td>I am a scientist at NASA that is discussing th...</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8</td>\n",
              "      <td>44</td>\n",
              "      <td>52.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17309</th>\n",
              "      <td>001ab80</td>\n",
              "      <td>People always wish they had the same technolog...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.00</td>\n",
              "      <td>5.5</td>\n",
              "      <td>7.5</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>104.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17310 rows × 3318 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      essay_id                                          full_text  num_para  \\\n",
              "0      000d118  Many people have car where they live. The thin...         1   \n",
              "1      000fe60  I am a scientist at NASA that is discussing th...         5   \n",
              "2      001ab80  People always wish they had the same technolog...         4   \n",
              "3      001bdc0  We all heard about Venus, the planet without a...         5   \n",
              "4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...         6   \n",
              "...        ...                                                ...       ...   \n",
              "17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...         1   \n",
              "17306  fffed3e  Venus is worthy place to study but dangerous. ...         4   \n",
              "17307  000d118  Many people have car where they live. The thin...         1   \n",
              "17308  000fe60  I am a scientist at NASA that is discussing th...         5   \n",
              "17309  001ab80  People always wish they had the same technolog...         4   \n",
              "\n",
              "       para_min_sent  para_q1_sent  para_median_sent  para_q3_sent  \\\n",
              "0                 13         13.00              13.0          13.0   \n",
              "1                  2          3.00               3.0           5.0   \n",
              "2                  4          4.00               5.5           7.5   \n",
              "3                  2          2.00               4.0           6.0   \n",
              "4                  1          1.25               3.0           4.0   \n",
              "...              ...           ...               ...           ...   \n",
              "17305             11         11.00              11.0          11.0   \n",
              "17306              1          1.00               1.5           3.5   \n",
              "17307             13         13.00              13.0          13.0   \n",
              "17308              2          3.00               3.0           5.0   \n",
              "17309              4          4.00               5.5           7.5   \n",
              "\n",
              "       para_max_sent  para_min_word  para_q1_word  ...  tfidf_3282  \\\n",
              "0                 13            545        545.00  ...         0.0   \n",
              "1                  8             44         52.00  ...         0.0   \n",
              "2                  9             92        104.75  ...         0.0   \n",
              "3                  7             25         69.00  ...         0.0   \n",
              "4                  4              3         22.00  ...         0.0   \n",
              "...              ...            ...           ...  ...         ...   \n",
              "17305             11            264        264.00  ...         0.0   \n",
              "17306              8              4         16.00  ...         0.0   \n",
              "17307             13            545        545.00  ...         0.0   \n",
              "17308              8             44         52.00  ...         0.0   \n",
              "17309              9             92        104.75  ...         0.0   \n",
              "\n",
              "       tfidf_3283  tfidf_3284  tfidf_3285  tfidf_3286  tfidf_3287  tfidf_3288  \\\n",
              "0        0.000000    0.000000         0.0         0.0         0.0    0.034733   \n",
              "1        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "2        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "3        0.043294    0.057518         0.0         0.0         0.0    0.000000   \n",
              "4        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "17305    0.023939    0.000000         0.0         0.0         0.0    0.000000   \n",
              "17306    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "17307    0.000000    0.000000         0.0         0.0         0.0    0.034733   \n",
              "17308    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "17309    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
              "\n",
              "       tfidf_3289  tfidf_3290  tfidf_3291  \n",
              "0        0.071065         0.0         0.0  \n",
              "1        0.000000         0.0         0.0  \n",
              "2        0.000000         0.0         0.0  \n",
              "3        0.000000         0.0         0.0  \n",
              "4        0.000000         0.0         0.0  \n",
              "...           ...         ...         ...  \n",
              "17305    0.000000         0.0         0.0  \n",
              "17306    0.000000         0.0         0.0  \n",
              "17307    0.071065         0.0         0.0  \n",
              "17308    0.000000         0.0         0.0  \n",
              "17309    0.000000         0.0         0.0  \n",
              "\n",
              "[17310 rows x 3318 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL4wA74jqqk_"
      },
      "source": [
        "# Xây dựng mô hình: SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_wFUa4qqlC"
      },
      "source": [
        "## _Chuẩn bị dữ liệu_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S3m9OxfgqqlC"
      },
      "outputs": [],
      "source": [
        "# Extract train and test from all_data\n",
        "train_processed = all_data.iloc[:train.shape[0], :].copy()\n",
        "train_processed['score'] = train['score']\n",
        "test_processed = all_data.iloc[train.shape[0]:, :].copy()\n",
        "test_processed.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2UMoViWoqqlC"
      },
      "outputs": [],
      "source": [
        "target_col = ['score']\n",
        "drop_cols = ['essay_id', 'full_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk2Vp6qSqqlC"
      },
      "source": [
        "## _Xây dựng kiểm tra chéo dữ liệu (cross validation) bằng StratifiedKFold_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "biBYwQg4qqlD"
      },
      "outputs": [],
      "source": [
        "# Build StratifiedKFold with 5 folds\n",
        "FOLDS = 3\n",
        "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(train_processed, train_processed['score'])):\n",
        "    train_processed.loc[val_index, 'fold'] = fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plL6MbpnqqlD"
      },
      "source": [
        "## _Xây dựng độ đo: Quadratic Weighted Kappa_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EqbCfY0nqqlD"
      },
      "outputs": [],
      "source": [
        "# Build Quadratic Weighted Kappa metric for evaluation SVM model\n",
        "def qwk_score(y_true, y_pred):\n",
        "    return cohen_kappa_score(y_true, y_pred.clip(1, 6).round(0), weights='quadratic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Vu7K3QqqlD"
      },
      "source": [
        "## _Huấn luyện dữ liệu với StratifiedKFold và đưa ra dự đoán_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1bupdInqqlD",
        "outputId": "e392ce06-62ae-49f8-dbeb-36467d533f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### Fold 1\n",
            "#########################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QWK score: 0.6877207089514217\n",
            "\n",
            "#########################\n",
            "### Fold 2\n",
            "#########################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QWK score: 0.6772280182365642\n",
            "\n",
            "#########################\n",
            "### Fold 3\n",
            "#########################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QWK score: 0.6881811138938584\n",
            "\n",
            "#########################\n",
            "Overall CV QWK score = 0.6844018549345805\n"
          ]
        }
      ],
      "source": [
        "train_preds = np.zeros(len(train_processed), dtype='float32')\n",
        "test_preds = np.zeros((len(test_processed),FOLDS), dtype='float32')\n",
        "score_qwk_list = []\n",
        "\n",
        "for fold in range(FOLDS):\n",
        "    print('#'*25)\n",
        "    print('### Fold',fold+1)\n",
        "    print('#'*25)\n",
        "\n",
        "    train_index = train_processed[\"fold\"] != fold\n",
        "    valid_index = train_processed[\"fold\"] == fold\n",
        "\n",
        "    X_train = train_processed[train_index].drop(columns=drop_cols+target_col+['fold'])\n",
        "    y_train = train_processed[train_index][target_col]\n",
        "    X_valid = train_processed[valid_index].drop(columns=drop_cols+target_col+['fold'])\n",
        "    y_valid = train_processed[valid_index][target_col]\n",
        "    X_test = test_processed.drop(columns=drop_cols)\n",
        "\n",
        "    model = SVR(C=10)\n",
        "    model.fit(X_train, y_train)\n",
        "    train_preds[valid_index] = model.predict(X_valid)\n",
        "    test_preds[:,fold] = model.predict(X_test)\n",
        "\n",
        "    score_qwk_list.append(qwk_score(y_valid, train_preds[valid_index]))  \n",
        "    print(f\"QWK score: {score_qwk_list[-1]}\")\n",
        "    print()\n",
        "\n",
        "print('#'*25)\n",
        "score = qwk_score(train_processed.score.values, train_preds)\n",
        "print('Overall CV QWK score =',score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kết quả dự đoán sẽ được tính trên trung bình của kết quả nhân trọng số QWK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3.01272981, 2.84280651, 4.06037431])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission_preds = np.dot(test_preds, score_qwk_list)/np.sum(score_qwk_list, axis=0)\n",
        "submission_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwsOY8bNqqlE"
      },
      "source": [
        "## _Xác định ngưỡng để làm tròn giá trị dự đoán của bài toán hồi quy_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J_jyiY8qqlE"
      },
      "source": [
        "Sử dụng mô hình hồi quy giúp cho việc học dữ liệu không bị overfitting. Vì thế vấn đề xảy ra khi sử dụng bài toán hồi quy cho biến dự đoán phân loại có thứ tự là xác định đúng được làm tròn số ở ngưỡng nào. Ví dụ, giá trị dự đoán là $1.6$, vậy ta sẽ chọn $1$ hay $2$? Ở bài toán này, chúng ta có 6 labels từ 1 đến 6, cho nên chúng ta sẽ có 5 ngưỡng cần xác định để làm tròn số. Chúng ta sử dụng QWK để làm độ đo tìm ra ngưỡng làm tròn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ABmWZx91qqlE"
      },
      "outputs": [],
      "source": [
        "def find_thresholds(true, pred, interrupt=50):\n",
        "    # Khởi tạo ngưỡng ban đầu\n",
        "    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
        "    pred_round = pd.cut(pred, [-np.inf] + threshold + [np.inf],\n",
        "                        labels=[1,2,3,4,5,6]).astype('int32')\n",
        "    best = cohen_kappa_score(true, pred_round, weights=\"quadratic\")\n",
        "\n",
        "    # Tìm lần lượt 5 ngưỡng\n",
        "    for k in range(5):\n",
        "        for sign in [1,-1]:\n",
        "            v = threshold[k]\n",
        "            threshold_test = threshold.copy()\n",
        "            stop = 0\n",
        "            while stop < interrupt:\n",
        "                # Tìm giá trị ngưỡng mới\n",
        "                v += sign * 0.001\n",
        "                threshold_test[k] = v\n",
        "                pred_round = pd.cut(pred, [-np.inf] + threshold_test + [np.inf],\n",
        "                                    labels=[1,2,3,4,5,6]).astype('int32')\n",
        "                metric = cohen_kappa_score(true, pred_round, weights=\"quadratic\")\n",
        "\n",
        "                # Phát hiện dừng sớm vòng lặp\n",
        "                if metric <= best:\n",
        "                    stop += 1\n",
        "                else:\n",
        "                    stop = 0\n",
        "                    best = metric\n",
        "                    threshold = threshold_test.copy()\n",
        "\n",
        "    # Trả kết quả ngưỡng tốt nhất\n",
        "    pred_round = pd.cut(pred, [-np.inf] + threshold + [np.inf],\n",
        "                        labels=[1,2,3,4,5,6]).astype('int32')\n",
        "    return threshold, cohen_kappa_score(true, pred_round, weights=\"quadratic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "afqhkesLqqlE"
      },
      "outputs": [],
      "source": [
        "thresholds, best_score = find_thresholds(train_processed.score.values, train_preds, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCgdhznG1XUV"
      },
      "source": [
        "Ta thấy ngưỡng làm tròn tốt nhất là:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvsJqVqFzjFn",
        "outputId": "b8400677-db1e-4b49-81a0-606ac794a40d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.9849999999999466,\n",
              " 2.626999999999986,\n",
              " 3.3880000000000123,\n",
              " 4.085999999999862,\n",
              " 4.826999999999775]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n_UXbpx1a1l"
      },
      "source": [
        "và cho điểm QWK là:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22NKYHf1zmFA",
        "outputId": "b70b1655-16dc-48c0-8832-a9562df51bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7196968696909072"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-ov-C-H1hJc"
      },
      "source": [
        "# Đưa ra dự đoán và submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Op9IM1eK1ofF",
        "outputId": "d1e27a15-b327-4747-a7b0-ce001658ff78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000d118</td>\n",
              "      <td>Many people have car where they live. The thin...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fe60</td>\n",
              "      <td>I am a scientist at NASA that is discussing th...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>001ab80</td>\n",
              "      <td>People always wish they had the same technolog...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  essay_id                                          full_text  score\n",
              "0  000d118  Many people have car where they live. The thin...      3\n",
              "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
              "2  001ab80  People always wish they had the same technolog...      4"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission_preds = pd.cut(submission_preds, [-np.inf] + thresholds + [np.inf], labels=[1,2,3,4,5,6]).astype('int32')\n",
        "submission = pd.concat([test, pd.Series(submission_preds, name='score')], axis=1)\n",
        "submission"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "min_ds-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
