{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        padding: 8px 8px;\n",
    "        background-image: linear-gradient(135deg, #c9f3ff, rgb(131, 218, 255));\n",
    "        font-weight: 700;\n",
    "        position: static;\n",
    "        text-align: center;\n",
    "        color: #006098;\n",
    "        font-size: 36px;\n",
    "    }\n",
    "    h2 {\n",
    "        font-weight: 700;\n",
    "        text-align: center;\n",
    "        font-style: italic;\n",
    "        font-size: 24px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div><h1>XÂY DỰNG MODEL</h1></div>\n",
    "<div><h2>SVM model via SVR</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khai báo thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/seaborn/_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.1)\n",
      "  from scipy.stats import gaussian_kde\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "# ! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# ! pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "# ! pip install spacy\n",
    "from spacy import load\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
    "# train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/test.csv')\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>1061742</td>\n",
       "      <td>There are millions of wrecks in the U.S. every...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>bd0afc4</td>\n",
       "      <td>I think that other people should join this Sea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2688e0d</td>\n",
       "      <td>I feel the elctoral college is overpowered and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>a984775</td>\n",
       "      <td>We should keep the Electoral College its a goo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>7d09ab7</td>\n",
       "      <td>I am all for Thomas Huang new invation. I thin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  score\n",
       "1035   1061742  There are millions of wrecks in the U.S. every...      4\n",
       "12836  bd0afc4  I think that other people should join this Sea...      2\n",
       "2564   2688e0d  I feel the elctoral college is overpowered and...      1\n",
       "11534  a984775  We should keep the Electoral College its a goo...      2\n",
       "8413   7d09ab7  I am all for Thomas Huang new invation. I thin...      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text\n",
       "0  000d118  Many people have car where they live. The thin...\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...\n",
       "2  001ab80  People always wish they had the same technolog..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13360</th>\n",
       "      <td>c41bf97</td>\n",
       "      <td>Many people think the face on mars was created...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16926</th>\n",
       "      <td>fa29121</td>\n",
       "      <td>Hello my name is Mr. Baker with NASA, and we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>a6526c3</td>\n",
       "      <td>Freedom without a car\\n\\nAround the world in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>f389a4c</td>\n",
       "      <td>Throughout the years, technology has improved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16589</th>\n",
       "      <td>f4ab05d</td>\n",
       "      <td>I whole heartedly believe driverless cars are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text\n",
       "13360  c41bf97  Many people think the face on mars was created...\n",
       "16926  fa29121  Hello my name is Mr. Baker with NASA, and we a...\n",
       "11291  a6526c3  Freedom without a car\\n\\nAround the world in d...\n",
       "16508  f389a4c  Throughout the years, technology has improved ...\n",
       "16589  f4ab05d  I whole heartedly believe driverless cars are ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove score from train and concate with test\n",
    "all_data = pd.concat([train.iloc[:,:-1], test], axis=0).reset_index(drop=True)\n",
    "all_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17310, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ loại bỏ các ký tự dư thừa và chuỗi không cung cấp nhiều ý nghĩa thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_str(text):\n",
    "    # Loại bỏ các username bắt đầu @\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    # Loại bỏ các thẻ HTML\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)\n",
    "    # Loại bỏ URL\n",
    "    text = [word for word in text.split() if not urlparse(word).scheme]\n",
    "    text = ' '.join(text)\n",
    "    # Loại bỏ dấu nháy đơn mà theo sau nó là chữ số\n",
    "    text = re.sub(\"'\\d+\", \"\", text)\n",
    "    # Loại bỏ các ký tự dư thừa như khoảng trắng, dấu chấm, dấu phẩy\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trích lọc đặc trưng từ dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta định nghĩa và cài đặt hàm đếm số lượng từ vựng bị sử dụng sai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spelling_errors(word_list):\n",
    "    return len(list(SpellChecker().unknown(word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, định nghĩa hàm loại bỏ dấu câu để thống kê thông số về từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, cài đặt hàm phân tách thành các đoạn văn, thành các câu để thống kê."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(text):\n",
    "    new_text = re.split('\\n\\n', text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cài đặt hàm trích lọc những từ khóa đóng góp quan trọng trong ý nghĩa của câu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text\n",
    "def filter_word(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần này, ta sẽ quan sát những giá trị thống kê từ văn bản:\n",
    "- Tổng số lượng đoạn văn;\n",
    "- Thống kê theo đoạn văn: Sau khi phân tách văn bản và xử lý dữ liệu, ta sẽ thống kê:\n",
    "    + Số lượng câu: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Tổng số lượng câu;\n",
    "- Thống kê theo câu: Sau khi phân tách thành các câu và xử lý dữ liệu, ta sẽ thống kê:\n",
    "    + Số lượng từ: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Tổng số lượng từ;\n",
    "- Tổng số lượng từ sau khi trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn;\n",
    "- Thống kê theo từ: Trích lọc những từ đóng góp ý nghĩa cho câu, đoạn văn:\n",
    "    + Số lượng ký tự: Giá trị nhỏ nhất, tứ phân vị thứ nhất, trung vị, tứ phân vị thứ ba, giá trị lớn nhất;\n",
    "- Số lượng từ loại sai chính tả trong đoạn văn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_engineering(paragraph):\n",
    "    _paragraph = list(map(remove_excess_str, paragraph))\n",
    "    _lst_sents = []\n",
    "    _lst_words = []\n",
    "    for para in _paragraph:\n",
    "        _lst_sents.append(len(sent_tokenize(para)))\n",
    "        _lst_words.append(len(word_tokenize(para)))\n",
    "    return len(_paragraph),\\\n",
    "            np.min(_lst_sents), np.quantile(_lst_sents, 0.25), np.median(_lst_sents), np.quantile(_lst_sents, 0.75), np.max(_lst_sents),\\\n",
    "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['num_para'],\\\n",
    "    all_data['para_min_sent'], all_data['para_q1_sent'], all_data['para_median_sent'], all_data['para_q3_sent'], all_data['para_max_sent'],\\\n",
    "    all_data['para_min_word'], all_data['para_q1_word'], all_data['para_median_word'], all_data['para_q3_word'], all_data['para_max_word'] = zip(*all_data['full_text'].map(split_paragraphs).map(paragraph_engineering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_engineering(sentence):\n",
    "    _lst_words = []\n",
    "    for sent in sentence:\n",
    "        _lst_words.append(len(word_tokenize(sent)))\n",
    "    return len(sentence),\\\n",
    "            np.min(_lst_words), np.quantile(_lst_words, 0.25), np.median(_lst_words), np.quantile(_lst_words, 0.75), np.max(_lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['num_sent'],\\\n",
    "    all_data['sent_min_word'], all_data['sent_q1_word'], all_data['sent_median_word'], all_data['sent_q3_word'], all_data['sent_max_word']\\\n",
    "        = zip(*all_data['full_text'].apply(remove_excess_str).map(sent_tokenize).map(sentence_engineering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_engineering(word):\n",
    "    _lst_chars = list(map(len, word))\n",
    "    return len(word), np.min(_lst_chars), np.quantile(_lst_chars, 0.25), np.median(_lst_chars), np.quantile(_lst_chars, 0.75), np.max(_lst_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_full_text = all_data['full_text'].map(remove_excess_str).map(remove_punctuation)\n",
    "word_full_text_no_stopwords = word_full_text.map(filter_word)\n",
    "# all_data['num_error'] = word_full_text.map(word_tokenize).map(count_spelling_errors)\n",
    "all_data['num_word'] = word_full_text.map(word_tokenize).map(len)\n",
    "all_data['num_word_no_stopwords'], all_data['word_min_chars'], all_data['word_q1_chars'], all_data['word_median_chars'], all_data['word_q3_chars'], all_data['word_max_chars'] = zip(*word_full_text_no_stopwords.map(word_engineering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Số lượng đặc trung sau khi rút trích thông tin từ dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17310, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trích lọc đặc trưng thông qua TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"TF-IDF là viết tắt của “Term Frequency, Inverse Document Frequency” - tạm dịch “Tần suất thuật ngữ, Tần suất tài liệu nghịch đảo”. Đó là một cách để chấm điểm tầm quan trọng của các từ (hoặc \\\"các thuật ngữ\\\") dựa trên tần suất xuất hiện của chúng xuất hiện trên nhiều tài liệu dựa trên quy tắc sau:\"\n",
    "- Nếu một từ xuất hiện thường xuyên trong tài liệu, điều đó rất quan trọng. Cho từ này điểm cao.\n",
    "- Nhưng nếu một từ xuất hiện trong nhiều tài liệu, thì đó không phải là mã định danh duy nhất. Cho từ đó điểm thấp.\n",
    "\n",
    "Do đó, những từ phổ biến như `the` và `for` xuất hiện trong nhiều tài liệu sẽ được scaled down. Các từ xuất hiện thường xuyên trong một tài liệu sẽ được scaled up.\n",
    "\n",
    "Với những giải thích trên, ta có công thức tính trọng số của một từ trong tài liệu trong ngữ liệu như sau:\n",
    "$$w_{i,j} = tf_{i,j} \\cdot idf_i = tf_{i,j} \\cdot log(\\frac {N}{df_i})$$\n",
    "\n",
    "Trong đó:\n",
    "- $tf_{i,j}$: Tần suất xuất hiện của i trong j\n",
    "- $N$: Tổng số tài liệu\n",
    "- $df_i$: Số tài liệu chứa i\n",
    "\n",
    "**Reference:** https://medium.com/analytics-vidhya/an-introduction-to-tf-idf-using-python-5f9d1a343f77\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo TfidfVectorizer với các tham số cụ thể\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=0.05,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# Sử dụng TfidfVectorizer cho dữ liệu\n",
    "tfidf = vectorizer.fit_transform([row for row in all_data['full_text']]).toarray()\n",
    "\n",
    "# Lưu kết quả\n",
    "colums_tfidf = [f'tfidf_{i}' for i in range(tfidf.shape[1])]\n",
    "tfidf_df = pd.DataFrame(tfidf, columns=colums_tfidf)\n",
    "all_data = pd.concat([all_data, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>num_para</th>\n",
       "      <th>para_min_sent</th>\n",
       "      <th>para_q1_sent</th>\n",
       "      <th>para_median_sent</th>\n",
       "      <th>para_q3_sent</th>\n",
       "      <th>para_max_sent</th>\n",
       "      <th>para_min_word</th>\n",
       "      <th>para_q1_word</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_3282</th>\n",
       "      <th>tfidf_3283</th>\n",
       "      <th>tfidf_3284</th>\n",
       "      <th>tfidf_3285</th>\n",
       "      <th>tfidf_3286</th>\n",
       "      <th>tfidf_3287</th>\n",
       "      <th>tfidf_3288</th>\n",
       "      <th>tfidf_3289</th>\n",
       "      <th>tfidf_3290</th>\n",
       "      <th>tfidf_3291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>545</td>\n",
       "      <td>545.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>104.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>69.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.057518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>22.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17305</th>\n",
       "      <td>fffb49b</td>\n",
       "      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>264</td>\n",
       "      <td>264.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17306</th>\n",
       "      <td>fffed3e</td>\n",
       "      <td>Venus is worthy place to study but dangerous. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>16.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17307</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>545</td>\n",
       "      <td>545.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17308</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17309</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>104.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17310 rows × 3318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  num_para  \\\n",
       "0      000d118  Many people have car where they live. The thin...         1   \n",
       "1      000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "2      001ab80  People always wish they had the same technolog...         4   \n",
       "3      001bdc0  We all heard about Venus, the planet without a...         5   \n",
       "4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...         6   \n",
       "...        ...                                                ...       ...   \n",
       "17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...         1   \n",
       "17306  fffed3e  Venus is worthy place to study but dangerous. ...         4   \n",
       "17307  000d118  Many people have car where they live. The thin...         1   \n",
       "17308  000fe60  I am a scientist at NASA that is discussing th...         5   \n",
       "17309  001ab80  People always wish they had the same technolog...         4   \n",
       "\n",
       "       para_min_sent  para_q1_sent  para_median_sent  para_q3_sent  \\\n",
       "0                 13         13.00              13.0          13.0   \n",
       "1                  2          3.00               3.0           5.0   \n",
       "2                  4          4.00               5.5           7.5   \n",
       "3                  2          2.00               4.0           6.0   \n",
       "4                  1          1.25               3.0           4.0   \n",
       "...              ...           ...               ...           ...   \n",
       "17305             11         11.00              11.0          11.0   \n",
       "17306              1          1.00               1.5           3.5   \n",
       "17307             13         13.00              13.0          13.0   \n",
       "17308              2          3.00               3.0           5.0   \n",
       "17309              4          4.00               5.5           7.5   \n",
       "\n",
       "       para_max_sent  para_min_word  para_q1_word  ...  tfidf_3282  \\\n",
       "0                 13            545        545.00  ...         0.0   \n",
       "1                  8             44         52.00  ...         0.0   \n",
       "2                  9             92        104.75  ...         0.0   \n",
       "3                  7             25         69.00  ...         0.0   \n",
       "4                  4              3         22.00  ...         0.0   \n",
       "...              ...            ...           ...  ...         ...   \n",
       "17305             11            264        264.00  ...         0.0   \n",
       "17306              8              4         16.00  ...         0.0   \n",
       "17307             13            545        545.00  ...         0.0   \n",
       "17308              8             44         52.00  ...         0.0   \n",
       "17309              9             92        104.75  ...         0.0   \n",
       "\n",
       "       tfidf_3283  tfidf_3284  tfidf_3285  tfidf_3286  tfidf_3287  tfidf_3288  \\\n",
       "0        0.000000    0.000000         0.0         0.0         0.0    0.034733   \n",
       "1        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "2        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "3        0.043294    0.057518         0.0         0.0         0.0    0.000000   \n",
       "4        0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "17305    0.023939    0.000000         0.0         0.0         0.0    0.000000   \n",
       "17306    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "17307    0.000000    0.000000         0.0         0.0         0.0    0.034733   \n",
       "17308    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "17309    0.000000    0.000000         0.0         0.0         0.0    0.000000   \n",
       "\n",
       "       tfidf_3289  tfidf_3290  tfidf_3291  \n",
       "0        0.071065         0.0         0.0  \n",
       "1        0.000000         0.0         0.0  \n",
       "2        0.000000         0.0         0.0  \n",
       "3        0.000000         0.0         0.0  \n",
       "4        0.000000         0.0         0.0  \n",
       "...           ...         ...         ...  \n",
       "17305    0.000000         0.0         0.0  \n",
       "17306    0.000000         0.0         0.0  \n",
       "17307    0.071065         0.0         0.0  \n",
       "17308    0.000000         0.0         0.0  \n",
       "17309    0.000000         0.0         0.0  \n",
       "\n",
       "[17310 rows x 3318 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chuẩn bị dữ liệu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train and test from all_data\n",
    "train_processed = all_data.iloc[:train.shape[0], :].copy()\n",
    "train_processed['score'] = train['score']\n",
    "test_processed = all_data.iloc[train.shape[0]:, :].copy()\n",
    "test_processed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "drop_cols = ['essay_id', 'full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Xây dựng kiểm tra chéo dữ liệu (cross validation) bằng StratifiedKFold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build StratifiedKFold with 5 folds\n",
    "FOLDS = 2\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_processed, train_processed['score'])):\n",
    "    train_processed.loc[val_index, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Xây dựng độ đo: Quadratic Weighted Kappa_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Quadratic Weighted Kappa metric for evaluation SVM model\n",
    "def qwk_score(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred.clip(1, 6).round(0), weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Huấn luyện dữ liệu với StratifiedKFold và đưa ra dự đoán_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'full_text', 'num_para', 'para_min_sent', 'para_q1_sent',\n",
       "       'para_median_sent', 'para_q3_sent', 'para_max_sent', 'para_min_word',\n",
       "       'para_q1_word',\n",
       "       ...\n",
       "       'tfidf_3282', 'tfidf_3283', 'tfidf_3284', 'tfidf_3285', 'tfidf_3286',\n",
       "       'tfidf_3287', 'tfidf_3288', 'tfidf_3289', 'tfidf_3290', 'tfidf_3291'],\n",
       "      dtype='object', length=3318)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'full_text', 'num_para', 'para_min_sent', 'para_q1_sent',\n",
       "       'para_median_sent', 'para_q3_sent', 'para_max_sent', 'para_min_word',\n",
       "       'para_q1_word',\n",
       "       ...\n",
       "       'tfidf_3283', 'tfidf_3284', 'tfidf_3285', 'tfidf_3286', 'tfidf_3287',\n",
       "       'tfidf_3288', 'tfidf_3289', 'tfidf_3290', 'tfidf_3291', 'fold'],\n",
       "      dtype='object', length=3319)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed.drop(columns=target_col).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.90450383, 2.84732938, 4.07502989])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=np.array([])\n",
    "temp = np.append(temp, [model.predict(X_test)])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK score: 0.6773548633062687\n",
      "\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK score: 0.684604062265085\n",
      "\n",
      "#########################\n",
      "Overall CV QWK score = 0.6810279418953384\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(train_processed), dtype='float32')\n",
    "test_preds = np.zeros((len(test_processed),FOLDS), dtype='float32')\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('#'*25)\n",
    "    \n",
    "    train_index = train_processed[\"fold\"] != fold\n",
    "    valid_index = train_processed[\"fold\"] == fold\n",
    "    \n",
    "    X_train = train_processed[train_index].drop(columns=drop_cols+target_col+['fold'])\n",
    "    y_train = train_processed[train_index][target_col]\n",
    "    X_valid = train_processed[valid_index].drop(columns=drop_cols+target_col+['fold'])\n",
    "    y_valid = train_processed[valid_index][target_col]\n",
    "    X_test = test_processed.drop(columns=drop_cols)\n",
    "    \n",
    "    model = SVR(C=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_preds[valid_index] = model.predict(X_valid)\n",
    "    test_preds[:,fold] = model.predict(X_test)\n",
    "\n",
    "    score = qwk_score(y_valid, train_preds[valid_index])    \n",
    "    print(f\"QWK score: {score}\")\n",
    "    print()\n",
    "    \n",
    "print('#'*25)\n",
    "score = qwk_score(train_processed.score.values, train_preds)\n",
    "print('Overall CV QWK score =',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 4, 5, 1], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut([1.2, 3.6, 4.2, 5.4, 1], [-np.inf] + [1.5, 2.5, 3.5, 4.5, 5.5] + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Xác định ngưỡng để làm tròn giá trị dự đoán của bài toán hồi quy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng mô hình hồi quy giúp cho việc học dữ liệu không bị overfitting. Vì thế vấn đề xảy ra khi sử dụng bài toán hồi quy cho biến dự đoán phân loại có thứ tự là xác định đúng được làm tròn số ở ngưỡng nào. Ví dụ, giá trị dự đoán là $1.6$, vậy ta sẽ chọn $1$ hay $2$? Ở bài toán này, chúng ta có 6 labels từ 1 đến 6, cho nên chúng ta sẽ có 5 ngưỡng cần xác định để làm tròn số. Chúng ta sử dụng QWK để làm độ đo tìm ra ngưỡng làm tròn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_thresholds(true, pred, interrupt=50):\n",
    "    # Khởi tạo ngưỡng ban đầu\n",
    "    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "    pred_round = pd.cut(pred, [-np.inf] + threshold + [np.inf],\n",
    "                        labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    best = cohen_kappa_score(true, pred_round, weights=\"quadratic\")\n",
    "\n",
    "    # Tìm lần lượt 5 ngưỡng\n",
    "    for k in range(5):\n",
    "        for sign in [1,-1]:\n",
    "            v = threshold[k]\n",
    "            threshold_test = threshold.copy()\n",
    "            stop = 0\n",
    "            while stop < interrupt:\n",
    "                # Tìm giá trị ngưỡng mới\n",
    "                v += sign * 0.01\n",
    "                threshold_test[k] = v\n",
    "                pred_round = pd.cut(pred, [-np.inf] + threshold_test + [np.inf],\n",
    "                                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "                metric = cohen_kappa_score(true, pred_round, weights=\"quadratic\")\n",
    "\n",
    "                # Phát hiện dừng sớm vòng lặp\n",
    "                if metric <= best:\n",
    "                    stop += 1\n",
    "                else:\n",
    "                    stop = 0\n",
    "                    best = metric\n",
    "                    threshold = threshold_test.copy()\n",
    "\n",
    "    # Trả kết quả ngưỡng tốt nhất\n",
    "    pred_round = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    return threshold, cohen_kappa_score(true, pred_round, weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, thresholds = find_thresholds(train_processed.score.values, train_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
