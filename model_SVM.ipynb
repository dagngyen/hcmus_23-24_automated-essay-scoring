{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        padding: 8px 8px;\n",
    "        background-image: linear-gradient(135deg, #c9f3ff, rgb(131, 218, 255));\n",
    "        font-weight: 700;\n",
    "        position: static;\n",
    "        text-align: center;\n",
    "        color: #006098;\n",
    "        font-size: 36px;\n",
    "    }\n",
    "    h2 {\n",
    "        font-weight: 700;\n",
    "        text-align: center;\n",
    "        font-style: italic;\n",
    "        font-size: 24px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div><h1>XÂY DỰNG MODEL</h1></div>\n",
    "<div><h2>SVM model via SVR</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khai báo thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagngyen/miniconda3/envs/min_ds-env/lib/python3.10/site-packages/seaborn/_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.1)\n",
      "  from scipy.stats import gaussian_kde\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "# ! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# ! pip install spacy\n",
    "from spacy import load\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
    "# train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/data/test.csv')\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu trên local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16606</th>\n",
       "      <td>f4d0ca7</td>\n",
       "      <td>The idea of reducing car usage is a good idea,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>1343a02</td>\n",
       "      <td>Dear fellow citizens I think you should join t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>6a5a5ad</td>\n",
       "      <td>The Electoral College is a process that should...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>b4c0303</td>\n",
       "      <td>The Electoral College is a process by which we...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14655</th>\n",
       "      <td>d7d0216</td>\n",
       "      <td>In \"The Challenge of Exploring Venus\" the auth...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  score\n",
       "16606  f4d0ca7  The idea of reducing car usage is a good idea,...      4\n",
       "1215   1343a02  Dear fellow citizens I think you should join t...      3\n",
       "7098   6a5a5ad  The Electoral College is a process that should...      5\n",
       "12284  b4c0303  The Electoral College is a process by which we...      4\n",
       "14655  d7d0216  In \"The Challenge of Exploring Venus\" the auth...      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text\n",
       "0  000d118  Many people have car where they live. The thin...\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...\n",
       "2  001ab80  People always wish they had the same technolog..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>35e2bbc</td>\n",
       "      <td>The Driverless people can cause problems. Many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>3f75229</td>\n",
       "      <td>The Mona Lisa is a very impressive mix of emot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6543</th>\n",
       "      <td>61d399c</td>\n",
       "      <td>Driveles cars are on the rise and should be al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17148</th>\n",
       "      <td>fd6e5a7</td>\n",
       "      <td>Driving in and of it self has always had a bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9259</th>\n",
       "      <td>88b80dc</td>\n",
       "      <td>The technology that they have produced is very...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text\n",
       "3631   35e2bbc  The Driverless people can cause problems. Many...\n",
       "4281   3f75229  The Mona Lisa is a very impressive mix of emot...\n",
       "6543   61d399c  Driveles cars are on the rise and should be al...\n",
       "17148  fd6e5a7  Driving in and of it self has always had a bad...\n",
       "9259   88b80dc  The technology that they have produced is very..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove score from train and concate with test\n",
    "all_data = pd.concat([train.iloc[:,:-1], test], axis=0).reset_index(drop=True)\n",
    "all_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Phân tách toàn bộ văn bản thành các đoạn văn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split full_text into paragraphs by '\\n\\n' with using regex\n",
    "# def split_paragraphs(text):\n",
    "#     new_text = re.split('\\n\\n', text)\n",
    "#     return new_text, len(new_text)\n",
    "# # def split_paragraphs(text):\n",
    "# #     new_text = text.split('\\n\\n')\n",
    "# #     return new_text, len(new_text)\n",
    "\n",
    "# train['paragraph'], train['paragraph_len'] = zip(*train['full_text'].map(split_paragraphs))\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Many people have car where they live. The thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "      <td>[I am a scientist at NASA that is discussing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>[People always wish they had the same technolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "      <td>[We all heard about Venus, the planet without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Dear, State Senator, This is a letter to argu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302</th>\n",
       "      <td>ffd378d</td>\n",
       "      <td>the story \" The Challenge of Exploing Venus \" ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[the story \" The Challenge of Exploing Venus \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>ffddf1f</td>\n",
       "      <td>Technology has changed a lot of ways that we l...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Technology has changed a lot of ways that we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17304</th>\n",
       "      <td>fff016d</td>\n",
       "      <td>If you don't like sitting around all day than ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[If you don't like sitting around all day than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17305</th>\n",
       "      <td>fffb49b</td>\n",
       "      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In \"The Challenge of Exporing Venus,\" the aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17306</th>\n",
       "      <td>fffed3e</td>\n",
       "      <td>Venus is worthy place to study but dangerous. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Venus is worthy place to study but dangerous....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17307 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  score  \\\n",
       "0      000d118  Many people have car where they live. The thin...      3   \n",
       "1      000fe60  I am a scientist at NASA that is discussing th...      3   \n",
       "2      001ab80  People always wish they had the same technolog...      4   \n",
       "3      001bdc0  We all heard about Venus, the planet without a...      4   \n",
       "4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3   \n",
       "...        ...                                                ...    ...   \n",
       "17302  ffd378d  the story \" The Challenge of Exploing Venus \" ...      2   \n",
       "17303  ffddf1f  Technology has changed a lot of ways that we l...      4   \n",
       "17304  fff016d  If you don't like sitting around all day than ...      2   \n",
       "17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...      1   \n",
       "17306  fffed3e  Venus is worthy place to study but dangerous. ...      2   \n",
       "\n",
       "                                               paragraph  \n",
       "0      [Many people have car where they live. The thi...  \n",
       "1      [I am a scientist at NASA that is discussing t...  \n",
       "2      [People always wish they had the same technolo...  \n",
       "3      [We all heard about Venus, the planet without ...  \n",
       "4      [Dear, State Senator, This is a letter to argu...  \n",
       "...                                                  ...  \n",
       "17302  [the story \" The Challenge of Exploing Venus \"...  \n",
       "17303  [Technology has changed a lot of ways that we ...  \n",
       "17304  [If you don't like sitting around all day than...  \n",
       "17305  [In \"The Challenge of Exporing Venus,\" the aut...  \n",
       "17306  [Venus is worthy place to study but dangerous....  \n",
       "\n",
       "[17307 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_paragraphs(text):\n",
    "    new_text = re.split('\\n\\n', text)\n",
    "    return new_text\n",
    "\n",
    "train['paragraph'] = train['full_text'].map(split_paragraphs)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Loại bỏ các ký tự dư thừa và chuỗi không cung cấp nhiều ý nghĩa thông tin_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_str(text):\n",
    "    # Loại bỏ các username bắt đầu @\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    # Loại bỏ các thẻ HTML\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)\n",
    "    # Loại bỏ URL\n",
    "    text = [word for word in text.split() if not urlparse(word).scheme]\n",
    "    text = ' '.join(text)\n",
    "    # Loại bỏ dấu nháy đơn theo sau số\n",
    "    text = re.sub(\"'\\d+\", \"\", text)\n",
    "    # Loại bỏ các ký tự dư thừa như khoảng trắng, dấu chấm, dấu phẩy\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trích lọc đặc trưng từ dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta định nghĩa và cài đặt hàm đếm số lượng từ vựng bị sử dụng sai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "with open('./embedding/words.txt', 'r') as file:\n",
    "    english_vocab = set(word.strip().lower() for word in file)\n",
    "    \n",
    "def count_spelling_errors(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
    "    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n",
    "    return spelling_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, định nghĩa hàm loại bỏ dấu câu để thống kê thông số về từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần này, ta sẽ quan sát những giá trị thống kê từ đoạn văn.\n",
    "- Đối với "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        (59, 13, 13.0, 13.0, 13.0, 13, 545, 545.0, 545...\n",
       "1        (40, 2, 3.0, 3.0, 5.0, 8, 44, 52.0, 81.0, 85.0...\n",
       "2        (58, 4, 4.0, 5.5, 7.5, 9, 92, 104.75, 149.5, 1...\n",
       "3        (65, 2, 2.0, 4.0, 6.0, 7, 25, 69.0, 75.0, 149....\n",
       "4        (52, 1, 1.25, 3.0, 4.0, 4, 3, 22.0, 87.0, 103....\n",
       "                               ...                        \n",
       "17305    (41, 11, 11.0, 11.0, 11.0, 11, 264, 264.0, 264...\n",
       "17306    (19, 1, 1.0, 1.5, 3.5, 8, 4, 16.0, 31.0, 57.0,...\n",
       "17307    (59, 13, 13.0, 13.0, 13.0, 13, 545, 545.0, 545...\n",
       "17308    (40, 2, 3.0, 3.0, 5.0, 8, 44, 52.0, 81.0, 85.0...\n",
       "17309    (58, 4, 4.0, 5.5, 7.5, 9, 92, 104.75, 149.5, 1...\n",
       "Name: full_text, Length: 17310, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = all_data['full_text'].map(split_paragraphs)\n",
    "paras = paragraph.iloc[0]\n",
    "\n",
    "def test_engi(paras):\n",
    "    num_errors = 0\n",
    "    lst_sentences = []\n",
    "    lst_words = []\n",
    "    for para in paras:\n",
    "        para = remove_excess_str(para)\n",
    "        num_errors += count_spelling_errors(para)\n",
    "        # Count number of sentences and words, then append to list\n",
    "        lst_sentences.append(len(sent_tokenize(para)))\n",
    "        lst_words.append(len(word_tokenize(para)))\n",
    "    return num_errors,\\\n",
    "            np.min(lst_sentences), np.quantile(lst_sentences, 0.25), np.median(lst_sentences), np.quantile(lst_sentences, 0.75), np.max(lst_sentences),\\\n",
    "            np.min(lst_words), np.quantile(lst_words, 0.25), np.median(lst_words), np.quantile(lst_words, 0.75), np.max(lst_words)\n",
    "paragraph.apply(lambda x: test_engi(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 13, 13.0, 13.0, 13.0, 13, 545, 545.0, 545.0, 545.0, 545)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(df):\n",
    "#     # Paragraph engineering\n",
    "#     paragraph = df['full_text'].map(split_paragraphs)\n",
    "#     df['num_paragraph'] = paragraph.map(len)\n",
    "#     def paragraph_engineering(lst):\n",
    "#         num_errors = 0\n",
    "#         lst_sentences = []\n",
    "#         lst_words = []\n",
    "#         for para in lst:\n",
    "#             para = remove_excess_str(para)\n",
    "#             num_errors += count_spelling_errors(para)\n",
    "#             # Count number of sentences and words, then append to list\n",
    "#             lst_sentences.append(len(sent_tokenize(para)))\n",
    "#             lst_words.append(len(word_tokenize(para)))\n",
    "#         return num_errors,\\\n",
    "#             np.min(lst_sentences), np.quantile(lst_sentences, 0.25), np.median(lst_sentences), np.quantile(lst_sentences, 0.75), np.max(lst_sentences),\\\n",
    "#             np.min(lst_words), np.quantile(lst_words, 0.25), np.median(lst_words), np.quantile(lst_words, 0.75), np.max(lst_words)\n",
    "#     df['num_errors'],\\\n",
    "#         df['para_min_sentence'], df['para_q1_sentence'], df['para_median_sentence'], df['para_q3_sentence'], df['para_max_sentence'],\\\n",
    "#             df['para_min_word'], df['para_q1_word'], df['para_median_word'], df['para_q3_word'], df['para_max_word'] = zip(*paragraph.map(paragraph_engineering))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_engineering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_engineering\u001b[49m(all_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_engineering' is not defined"
     ]
    }
   ],
   "source": [
    "feature_engineering(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xin chào Việt Nam!...'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"    Xin  chào Việt Nam!... \"\n",
    "\n",
    "text = [word.strip() for word in text.split()]\n",
    "text = ' '.join(text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad escape \\d at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/min_ds-env/lib/python3.10/sre_parse.py:1051\u001b[0m, in \u001b[0;36mparse_template\u001b[0;34m(source, state)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1051\u001b[0m     this \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(\u001b[43mESCAPES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\\\d'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Xin  chào Việt Nam... \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m33 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mremove_excess_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 11\u001b[0m, in \u001b[0;36mremove_excess_str\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Loại bỏ dấu nháy đơn theo sau số\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loại bỏ các ký tự dư thừa như khoảng trắng, dấu chấm, dấu phẩy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m~/miniconda3/envs/min_ds-env/lib/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/min_ds-env/lib/python3.10/re.py:326\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_repl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m template[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/min_ds-env/lib/python3.10/re.py:317\u001b[0m, in \u001b[0;36m_compile_repl\u001b[0;34m(repl, pattern)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(_MAXCACHE)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_repl\u001b[39m(repl, pattern):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# internal: compile replacement pattern\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/min_ds-env/lib/python3.10/sre_parse.py:1054\u001b[0m, in \u001b[0;36mparse_template\u001b[0;34m(source, state)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   1053\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ASCIILETTERS:\n\u001b[0;32m-> 1054\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m s\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad escape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m this, \u001b[38;5;28mlen\u001b[39m(this))\n\u001b[1;32m   1055\u001b[0m         lappend(this)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31merror\u001b[0m: bad escape \\d at position 0"
     ]
    }
   ],
   "source": [
    "text = \"    Xin  chào Việt Nam... '33 \"\n",
    "remove_excess_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    print(words)\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'learning', 'natural', 'language', 'processing', '.', 'we', 'does', 'not', 'have', 'vietnam', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['learning', 'natural', 'language', 'processing', 'vietnam']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"I am learning Natural Language Processing. We does not have Vietnam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you're\", 'each', \"weren't\", 'weren', 'than', 'after', 'are', 'own', 'she', 'be', 'wouldn', 'the', 'while', 'too', 'do', \"shouldn't\", 'our', 'which', 'down', 'themselves', 'ain', \"needn't\", 'because', \"couldn't\", 'does', \"isn't\", 'won', 'once', 'as', 'we', 'isn', \"don't\", 'out', 'now', 'd', \"wouldn't\", 'how', 'this', 't', 'me', 'by', 'under', 'is', 'such', 'here', 'doesn', 'why', 'only', 'ma', \"mightn't\", 'her', 'nor', 'between', 'there', 'your', 'no', 'y', 'needn', \"you'd\", 'so', 'before', 'm', 's', 'been', 'ourselves', \"aren't\", 'herself', 'couldn', 'did', 'himself', 'those', 'during', 'a', 'through', 'until', 'it', 'just', \"should've\", \"didn't\", 'up', 'from', \"shan't\", 'should', 'into', 'of', \"hadn't\", 'will', 'them', 'below', 'further', 'its', 'doing', 'his', 'they', 'what', 'both', 'don', 'has', 'being', 'very', 'myself', 'any', \"hasn't\", \"you've\", 'yours', 'hers', 'with', 'when', 'my', 've', 'all', 'he', 'didn', 'him', 'if', 'these', 'not', \"it's\", 'll', 'who', 'have', 'an', 'other', 'aren', 'at', 'hadn', 'shan', 'shouldn', 'mightn', 'and', 'that', 'over', \"won't\", \"you'll\", 'wasn', \"that'll\", 'but', 'you', 'to', 'hasn', 'where', 'in', \"doesn't\", 'off', 're', 'about', \"she's\", 'again', 'o', 'had', \"mustn't\", 'itself', 'yourself', 'were', 'theirs', 'haven', 'ours', 'having', 'most', 'some', 'i', 'yourselves', 'more', 'few', 'can', 'or', 'am', 'their', 'was', 'for', 'on', 'then', 'mustn', \"haven't\", \"wasn't\", 'same', 'against', 'above', 'whom'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am learning Natural Language Processing We does not have Vietnam'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from the input text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with punctuation removed.\n",
    "    \"\"\"\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "remove_punctuation(\"I am learning Natural Language Processing. We does not have Vietnam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "with open('./embedding/words.txt', 'r') as file:\n",
    "    english_vocab = set(word.strip().lower() for word in file)\n",
    "    \n",
    "def count_spelling_errors(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
    "    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n",
    "    return spelling_errors\n",
    "\n",
    "count_spelling_errors(\"I am learning Natural Language Processing. We does not have Vietnam.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
